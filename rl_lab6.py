# -*- coding: utf-8 -*-
"""RL_Lab6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bApKfxhEF62MqnFv5CuAMscw_0GK2cJI
"""

import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# ------------------------------------------------
# Lab 6: Bootstrapping Techniques in RL
# TD(0) and SARSA with multiple visualizations
# ------------------------------------------------

env = gym.make("FrozenLake-v1", is_slippery=True)

alpha = 0.1
gamma = 0.99
epsilon = 0.1
episodes = 2000

# ------------------------------------------------
# 1. TD(0) State Value Learning
# ------------------------------------------------

V = np.zeros(env.observation_space.n)
td_errors = []
episode_lengths = []

for ep in range(episodes):
    state, _ = env.reset()
    done = False
    length = 0

    while not done:
        action = env.action_space.sample()
        next_state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated

        td_target = reward + gamma * V[next_state]
        td_error = td_target - V[state]
        V[state] += alpha * td_error
        td_errors.append(td_error)

        state = next_state
        length += 1

    episode_lengths.append(length)


# ------------------------------------------------
# 2. SARSA for Q Learning
# ------------------------------------------------

Q = np.zeros((env.observation_space.n, env.action_space.n))
sarsa_rewards = []

def epsilon_greedy(state):
    if np.random.rand() < epsilon:
        return env.action_space.sample()
    return np.argmax(Q[state])

for ep in range(episodes):
    state, _ = env.reset()
    action = epsilon_greedy(state)
    total_r = 0
    done = False

    while not done:
        next_state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        next_action = epsilon_greedy(next_state)

        td_target = reward + gamma * Q[next_state][next_action]
        td_error = td_target - Q[state][action]
        Q[state][action] += alpha * td_error

        total_r += reward
        state = next_state
        action = next_action

    sarsa_rewards.append(total_r)

# ------------------------------------------------
# Visualization Section
# ------------------------------------------------

plt.figure(figsize=(16,10))

# 1. TD Error Trend
plt.subplot(3, 2, 1)
plt.plot(td_errors)
plt.title("TD(0) Temporal Difference Error")
plt.xlabel("Step")
plt.ylabel("TD Error")

# 2. SARSA reward trend
plt.subplot(3, 2, 2)
plt.plot(np.convolve(sarsa_rewards, np.ones(50)/50, mode='valid'))
plt.title("SARSA Average Reward Trend")
plt.xlabel("Episodes")
plt.ylabel("Avg Reward")

# 3. Value Function Heatmap
plt.subplot(3, 2, 3)
sns.heatmap(V.reshape(4,4), annot=True, cmap="Blues")
plt.title("TD(0) Value Function Heatmap")

# 4. Q Table Heatmap
plt.subplot(3, 2, 4)
sns.heatmap(Q, annot=False, cmap="YlGnBu")
plt.title("SARSA Q Table Heatmap")

# 5. Policy Arrows (from Q)
policy = np.argmax(Q, axis=1).reshape(4,4)
arrows = {0:"←", 1:"↓", 2:"→", 3:"↑"}

plt.subplot(3, 2, 5)
plt.title("Derived Policy (Arrows)")
for i in range(4):
    for j in range(4):
        plt.text(j + 0.5, i + 0.5, arrows[policy[i,j]], fontsize=20, ha='center')
plt.xlim(0, 4)
plt.ylim(0, 4)
plt.gca().invert_yaxis()
plt.xticks([])
plt.yticks([])
plt.grid()

# 6. Episode length trend (TD)
plt.subplot(3, 2, 6)
plt.plot(episode_lengths)
plt.title("Episode Length Trend (TD(0))")
plt.xlabel("Episodes")
plt.ylabel("Length")

plt.tight_layout()
plt.show()

print("\nFinal State Values (TD):\n", V.reshape(4,4))
print("\nFinal Q Table (SARSA):\n", Q)