# -*- coding: utf-8 -*-
"""RL_Lab7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16g43iTQwBcaE4bmw2Peh9EVqq-GqscKa
"""

import numpy as np
import matplotlib.pyplot as plt

# -------------------------------------
# Environment: 1D Random Walk
# -------------------------------------

class RandomWalkEnv:
    def __init__(self, num_states=5):
        self.num_states = num_states
        self.terminal_left = 0
        self.terminal_right = num_states + 1
        self.state = None

    def reset(self):
        self.state = self.num_states // 2 + 1  # start in the middle (state 3)
        return self.state

    def step(self, action):
        next_state = self.state + action

        if next_state == self.terminal_right:
            reward = 1.0
            done = True
        elif next_state == self.terminal_left:
            reward = 0.0
            done = True
        else:
            reward = 0.0
            done = False

        self.state = next_state
        return next_state, reward, done


# -------------------------------------
# TD(0) Algorithm
# -------------------------------------

def td_zero(env, alpha=0.1, gamma=1.0, num_episodes=1000):
    V = np.zeros(env.num_states + 2)  # include terminal states

    # store history of V for visualization
    V_history = []

    for episode in range(num_episodes):
        state = env.reset()
        done = False

        while not done:
            action = np.random.choice([-1, 1])  # random policy
            next_state, reward, done = env.step(action)

            td_target = reward + gamma * V[next_state]
            td_error = td_target - V[state]

            V[state] = V[state] + alpha * td_error

            state = next_state

        # record snapshot after each episode
        V_history.append(V.copy())

    return V, np.array(V_history)


# -------------------------------------
# Visualization Functions
# -------------------------------------

def plot_final_values(V):
    states = np.arange(len(V))
    plt.figure(figsize=(7, 4))
    plt.plot(states, V, marker='o')
    plt.title("Final State Value Estimates using TD(0)")
    plt.xlabel("State")
    plt.ylabel("Value V(s)")
    plt.grid(True)
    plt.show()


def plot_value_progression(V_history):
    plt.figure(figsize=(8, 5))

    for i in range(1, len(V_history), len(V_history)//10):
        plt.plot(V_history[i, :], label=f"Episode {i}")

    plt.title("Value Function Progression Over Episodes")
    plt.xlabel("State")
    plt.ylabel("Value V(s)")
    plt.legend()
    plt.grid(True)
    plt.show()


# -------------------------------------
# Run TD(0) + Plot
# -------------------------------------

if __name__ == "__main__":
    env = RandomWalkEnv(num_states=5)

    V, V_history = td_zero(env, alpha=0.1, gamma=1.0, num_episodes=500)

    print("Final value estimates:")
    for s in range(len(V)):
        print(f"State {s}: {V[s]:.3f}")

    plot_final_values(V)
    plot_value_progression(V_history)