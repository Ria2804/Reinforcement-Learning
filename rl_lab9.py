# -*- coding: utf-8 -*-
"""RL_lab9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h-GlofJBnjkCg6LnOKLJ4rXtEqZuE14U
"""

import numpy as np
import matplotlib.pyplot as plt

# =====================================================
# Simple Random Walk Environment
# =====================================================
class RandomWalkEnv:
    """
    States: 0 and N-1 are terminal.
    Start in the middle. At each step, move left or right.
    Left terminal reward = -1
    Right terminal reward = +1
    """

    def __init__(self, n_states=7):
        self.n_states = n_states
        self.start_state = n_states // 2
        self.state = self.start_state

    def reset(self):
        self.state = self.start_state
        return self.state

    def step(self, action=None):
        if action is None:
            action = np.random.choice([0, 1])

        next_state = self.state - 1 if action == 0 else self.state + 1

        reward = 0
        done = False

        if next_state == 0:
            reward = -1
            done = True
        elif next_state == self.n_states - 1:
            reward = +1
            done = True

        self.state = next_state
        return next_state, reward, done, {}


# =====================================================
# TD(Î») algorithm with eligibility traces
# =====================================================
def td_lambda(env, n_states, alpha=0.1, gamma=0.99, lam=0.8, episodes=200):
    V = np.zeros(n_states)
    all_V = []              # store value function after every episode

    for ep in range(episodes):
        s = env.reset()
        e = np.zeros(n_states)
        done = False

        while not done:
            a = np.random.choice([0, 1])
            s_next, r, done, _ = env.step(a)

            # TD Error
            td_target = r if done else r + gamma * V[s_next]
            delta = td_target - V[s]

            # Eligibility update
            e[s] += 1

            # Update all states
            V += alpha * delta * e

            # Decay eligibility traces
            e *= gamma * lam

            s = s_next

        all_V.append(V.copy())

    return V, np.array(all_V)


# =====================================================
# Visualization: Value Function
# =====================================================
def plot_values(V, title="Final State Value Estimates"):
    plt.figure(figsize=(7,4))
    plt.plot(V, marker='o')
    plt.title(title)
    plt.xlabel("State")
    plt.ylabel("Value")
    plt.grid(True)
    plt.show()


# =====================================================
# Visualization: Heatmap of Value Evolution
# =====================================================
def plot_heatmap(all_V):
    plt.figure(figsize=(8,6))
    plt.imshow(all_V, cmap='viridis', aspect='auto')
    plt.colorbar(label="Value Estimate")
    plt.title("Learning Progress Across Episodes")
    plt.xlabel("State")
    plt.ylabel("Episode")
    plt.show()


# =====================================================
# Visualization: Eligibility Trace Example
# =====================================================
def visualize_eligibility_trace(n_states, gamma=0.99, lam=0.8, steps=10):
    e = np.zeros(n_states)
    history = []

    for step in range(steps):
        visited = np.random.randint(1, n_states-1)
        e[visited] += 1
        e *= gamma * lam
        history.append(e.copy())

    plt.figure(figsize=(8,5))
    plt.imshow(history, cmap="magma", aspect="auto")
    plt.colorbar(label="Eligibility")
    plt.title("Example Eligibility Trace Decay")
    plt.xlabel("State")
    plt.ylabel("Time Step")
    plt.show()


# =====================================================
# MAIN DRIVER CODE
# =====================================================
if __name__ == "__main__":
    env = RandomWalkEnv(n_states=7)

    final_V, all_V = td_lambda(
        env,
        n_states=env.n_states,
        alpha=0.1,
        gamma=0.99,
        lam=0.8,
        episodes=250
    )

    print("Final learned values:")
    print(final_V)

    # Plot results
    plot_values(final_V)
    plot_heatmap(all_V)
    visualize_eligibility_trace(env.n_states)