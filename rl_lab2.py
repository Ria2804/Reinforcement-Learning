# -*- coding: utf-8 -*-
"""RL_Lab2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19MeQEmLJiPzbtTFYlhz7Tg1jWAapACs9
"""

import numpy as np
import matplotlib.pyplot as plt

# -------------------------------
# 1️⃣ Environment (K-Armed Bandit)
# -------------------------------
class BanditEnv:
    def __init__(self, k=5):
        self.k = k
        self.q_true = np.random.normal(0, 1, k)  # true mean rewards

    def pull(self, action):
        # stochastic reward with some noise
        return np.random.normal(self.q_true[action], 1)

# -------------------------------
# 2️⃣ Algorithms
# -------------------------------

# --- UCB (Upper Confidence Bound) ---
class UCB:
    def __init__(self, k, c=2):
        self.k = k
        self.c = c
        self.Q = np.zeros(k)
        self.N = np.zeros(k)
        self.t = 0

    def select_action(self, t):
        self.t += 1
        with np.errstate(divide='ignore'):
            ucb_values = self.Q + self.c * np.sqrt(np.log(self.t + 1) / (self.N + 1e-5))
        return np.argmax(ucb_values)

    def update(self, action, reward):
        self.N[action] += 1
        self.Q[action] += (reward - self.Q[action]) / self.N[action]


# --- KL-UCB (Kullback-Leibler UCB Approximation) ---
class KLUCB:
    def __init__(self, k, c=3):
        self.k = k
        self.c = c
        self.Q = np.zeros(k)
        self.N = np.zeros(k)
        self.t = 0

    def kl_divergence(self, p, q):
        eps = 1e-8
        return p * np.log((p + eps) / (q + eps)) + (1 - p) * np.log((1 - p + eps) / (1 - q + eps))

    def select_action(self, t):
        self.t += 1
        if 0 in self.N:
            return np.argmin(self.N)
        upper_bounds = self.Q + self.c * np.sqrt(np.log(self.t) / (self.N + 1e-5))
        return np.argmax(upper_bounds)

    def update(self, action, reward):
        self.N[action] += 1
        self.Q[action] += (reward - self.Q[action]) / self.N[action]


# --- Gradient Bandit (Policy-based) ---
class GradientBandit:
    def __init__(self, k, alpha=0.1):
        self.k = k
        self.alpha = alpha
        self.H = np.zeros(k)  # preferences
        self.avg_reward = 0
        self.t = 0

    def select_action(self):
        probs = np.exp(self.H) / np.sum(np.exp(self.H))
        action = np.random.choice(self.k, p=probs)
        return action, probs

    def update(self, action, reward, probs):
        self.t += 1
        self.avg_reward += (reward - self.avg_reward) / self.t
        baseline = self.avg_reward
        for a in range(self.k):
            if a == action:
                self.H[a] += self.alpha * (reward - baseline) * (1 - probs[a])
            else:
                self.H[a] -= self.alpha * (reward - baseline) * probs[a]

# -------------------------------
# 3️⃣ Simulation
# -------------------------------
def run_experiment(bandit_class, steps=1000, runs=100):
    k = 5
    avg_rewards = np.zeros(steps)
    action_counts = np.zeros((runs, steps))
    prob_history = []  # store probability evolution (only for Gradient Bandit)

    for run in range(runs):
        env = BanditEnv(k)
        agent = bandit_class(k)
        rewards = []
        for t in range(steps):
            if isinstance(agent, GradientBandit):
                action, probs = agent.select_action()
                reward = env.pull(action)
                agent.update(action, reward, probs)
                prob_history.append(probs)
            else:
                action = agent.select_action(t)
                reward = env.pull(action)
                agent.update(action, reward)
            rewards.append(reward)
            action_counts[run, t] = action
        avg_rewards += np.array(rewards)

    avg_rewards /= runs
    return avg_rewards, action_counts, np.array(prob_history)

# -------------------------------
# 4️⃣ Run and Visualize
# -------------------------------
steps = 1000
rewards_ucb, act_ucb, _ = run_experiment(UCB, steps)
rewards_klucb, act_klucb, _ = run_experiment(KLUCB, steps)
rewards_grad, act_grad, probs_grad = run_experiment(GradientBandit, steps)

# ---- Reward Comparison ----
plt.figure(figsize=(10,6))
plt.plot(rewards_ucb, label='UCB')
plt.plot(rewards_klucb, label='KL-UCB')
plt.plot(rewards_grad, label='Gradient Bandit')
plt.xlabel('Steps')
plt.ylabel('Average Reward')
plt.title('Comparison of Bandit Algorithms')
plt.legend()
plt.grid(True)
plt.show()

# ---- Action Selection Heatmaps ----
plt.figure(figsize=(12, 4))
plt.subplot(1, 3, 1)
plt.imshow(act_ucb, aspect='auto', cmap='tab20')
plt.title("UCB Action Selections")
plt.xlabel("Steps")
plt.ylabel("Run")

plt.subplot(1, 3, 2)
plt.imshow(act_klucb, aspect='auto', cmap='tab20')
plt.title("KL-UCB Action Selections")
plt.xlabel("Steps")

plt.subplot(1, 3, 3)
plt.imshow(act_grad, aspect='auto', cmap='tab20')
plt.title("Gradient Bandit Action Selections")
plt.xlabel("Steps")

plt.tight_layout()
plt.show()

# ---- Gradient Bandit: Probability Evolution ----
if len(probs_grad) > 0:
    avg_probs = np.mean(probs_grad.reshape(-1, steps, 5), axis=0)
    plt.figure(figsize=(8,5))
    for i in range(avg_probs.shape[1]):
        plt.plot(avg_probs[:, i], label=f'Action {i}')
    plt.xlabel('Steps')
    plt.ylabel('Selection Probability')
    plt.title('Gradient Bandit: Policy Probability Evolution')
    plt.legend()
    plt.grid(True)
    plt.show()