# -*- coding: utf-8 -*-
"""RL_Lab5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GAEqqLwQwUoF0sJcUGz7yqkXXLSw-cBE
"""

"""
Reinforcement Learning Lab 5
Q-Learning with Visualizations

"""

import numpy as np
import matplotlib.pyplot as plt

# --------------------------------------------
# Environment Setup (GridWorld)
# --------------------------------------------
class GridWorld:
    def __init__(self):
        # 5x5 grid
        self.size = 5
        # Start & Goal
        self.start = (0, 0)
        self.goal = (4, 4)
        # Walls (optional obstacles)
        self.walls = {(1,2), (2,2), (3,1)}

        self.state = self.start
        self.max_steps = 50

    def reset(self):
        self.state = self.start
        self.steps = 0
        return self._to_state(self.state)

    def _to_state(self, pos):
        return pos[0] * self.size + pos[1]

    def step(self, action):
        # Actions: 0:UP, 1:RIGHT, 2:DOWN, 3:LEFT
        moves = [(-1,0), (0,1), (1,0), (0,-1)]
        r, c = self.state
        dr, dc = moves[action]
        nr, nc = r + dr, c + dc

        self.steps += 1

        # Validate move
        if (nr < 0 or nr >= self.size or nc < 0 or nc >= self.size or (nr,nc) in self.walls):
            # Penalize bump
            reward = -1
            done = False
        else:
            self.state = (nr,nc)
            if self.state == self.goal:
                return self._to_state(self.state), 20, True, {}
            reward = -1
            done = False

        if self.steps >= self.max_steps:
            done = True

        return self._to_state(self.state), reward, done, {}

# --------------------------------------------
# Q-Learning Agent
# --------------------------------------------
class QLearningAgent:
    def __init__(self, n_states, n_actions, alpha=0.1, gamma=0.99, epsilon=1.0, decay=0.995):
        self.Q = np.zeros((n_states, n_actions))
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
        self.decay = decay

    def select_action(self, state):
        if np.random.rand() < self.epsilon:
            return np.random.randint(4)
        return np.argmax(self.Q[state])

    def update(self, s, a, r, s_next, done):
        target = r + (0 if done else self.gamma * np.max(self.Q[s_next]))
        self.Q[s,a] += self.alpha * (target - self.Q[s,a])

# --------------------------------------------
# Training
# --------------------------------------------
env = GridWorld()
agent = QLearningAgent(n_states=25, n_actions=4)

episodes = 500
rewards = []

for ep in range(episodes):
    s = env.reset()
    done = False
    total_r = 0

    while not done:
        a = agent.select_action(s)
        s_next, r, done, _ = env.step(a)
        agent.update(s, a, r, s_next, done)
        s = s_next
        total_r += r

    agent.epsilon = max(0.05, agent.epsilon * agent.decay)
    rewards.append(total_r)

print("Training complete ✅")

# --------------------------------------------
# Visualization 1: Reward Plot
# --------------------------------------------
plt.figure(figsize=(7,4))
plt.plot(rewards)
plt.title("Episode Reward Curve")
plt.xlabel("Episodes")
plt.ylabel("Total Episode Reward")
plt.grid(True)
plt.show()

# --------------------------------------------
# Visualization 2: State-Value Heatmap (V(s) = max(Q(s,a)))
# --------------------------------------------
values = np.max(agent.Q, axis=1).reshape(5,5)

plt.figure(figsize=(5,5))
plt.imshow(values, cmap='viridis')
plt.colorbar(label="Value")
plt.title("State Value Heatmap")
plt.show()

# --------------------------------------------
# Visualization 3: Policy Arrows
# --------------------------------------------
arrows = {0:'↑', 1:'→', 2:'↓', 3:'←'}

print("\nLearned Policy (best action per cell):\n")
for i in range(5):
    row = ""
    for j in range(5):
        if (i,j) == env.goal:
            row += " G  "
        elif (i,j) in env.walls:
            row += " #  "
        else:
            s = i*5 + j
            best_action = np.argmax(agent.Q[s])
            row += " " + arrows[best_action] + "  "
    print(row)
print()