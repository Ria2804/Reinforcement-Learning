# -*- coding: utf-8 -*-
"""RL_LAB8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vKQQy8beLevfsWD5hCPGih28kEpGom4U
"""

# ==========================================================
# Lab 8: SARSA Algorithm for Action-Value Estimation
# Environment: FrozenLake-v1 (4x4)
# ==========================================================

import numpy as np
import random
import matplotlib.pyplot as plt
import seaborn as sns
import gymnasium as gym

# ----------------------------------------------------------
# Epsilon-greedy action selection
# ----------------------------------------------------------
def epsilon_greedy(Q, state, n_actions, epsilon):
    if random.random() < epsilon:
        return random.randint(0, n_actions - 1)
    return np.argmax(Q[state])


# ----------------------------------------------------------
# SARSA Implementation
# ----------------------------------------------------------
def sarsa(env, episodes=1000, alpha=0.1, gamma=0.99, epsilon=0.1):
    n_states = env.observation_space.n
    n_actions = env.action_space.n

    # Q-table initialized with zeros
    Q = np.zeros((n_states, n_actions))

    episode_rewards = []

    for ep in range(episodes):
        state, info = env.reset()

        # Initial action
        action = epsilon_greedy(Q, state, n_actions, epsilon)
        total_reward = 0
        done = False

        while not done:
            next_state, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            total_reward += reward

            if not done:
                next_action = epsilon_greedy(Q, next_state, n_actions, epsilon)
            else:
                next_action = 0

            # SARSA update rule
            td_target = reward + (0 if done else gamma * Q[next_state, next_action])
            td_error = td_target - Q[state, action]
            Q[state, action] += alpha * td_error

            state = next_state
            action = next_action

        episode_rewards.append(total_reward)

    return Q, episode_rewards


# ----------------------------------------------------------
# Visualization: Reward Curve
# ----------------------------------------------------------
def plot_rewards(rewards):
    plt.figure(figsize=(8,4))
    plt.plot(rewards, label="Episode Reward")
    plt.title("SARSA Learning Curve")
    plt.xlabel("Episodes")
    plt.ylabel("Reward")
    plt.grid(True)
    plt.legend()
    plt.show()


# ----------------------------------------------------------
# Visualization: Q-table Heatmaps
# ----------------------------------------------------------
def plot_q_values(Q):
    plt.figure(figsize=(12,6))
    for action in range(Q.shape[1]):
        plt.subplot(1, Q.shape[1], action+1)
        q_grid = Q[:, action].reshape(4,4)
        sns.heatmap(q_grid, annot=True, cmap="viridis", cbar=False, square=True)
        plt.title(f"Action {action}")
    plt.suptitle("Learned Q Values for Each Action")
    plt.show()


# ----------------------------------------------------------
# MAIN PROGRAM
# ----------------------------------------------------------
if __name__ == "__main__":
    env = gym.make("FrozenLake-v1", is_slippery=True)

    Q, rewards = sarsa(env, episodes=2000, alpha=0.1, gamma=0.99, epsilon=0.15)

    print("Training complete. Final Q-table:")
    print(Q)

    plot_rewards(rewards)
    plot_q_values(Q)