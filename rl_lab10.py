# -*- coding: utf-8 -*-
"""RL_lab10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zs_Dj7MmuzN3lB_qSn0GHqakv23nG6Z1
"""

import numpy as np
import matplotlib.pyplot as plt
import random

# ===========================================
# Gridworld Environment
# ===========================================

class GridWorld:
    def __init__(self, size=5):
        self.size = size
        self.state_space = size * size
        self.action_space = 4   # up, down, left, right
        self.reset()

    def state_to_pos(self, s):
        return s // self.size, s % self.size

    def pos_to_state(self, r, c):
        return r * self.size + c

    def reset(self):
        self.agent_pos = (0, 0)
        return self.pos_to_state(0, 0)

    def step(self, action):
        r, c = self.agent_pos

        if action == 0:     # up
            r = max(0, r - 1)
        elif action == 1:   # down
            r = min(self.size - 1, r + 1)
        elif action == 2:   # left
            c = max(0, c - 1)
        elif action == 3:   # right
            c = min(self.size - 1, c + 1)

        self.agent_pos = (r, c)
        next_state = self.pos_to_state(r, c)

        reward = -1
        done = False

        if (r, c) == (self.size - 1, self.size - 1):
            reward = 10
            done = True

        return next_state, reward, done, {}

# ===========================================
# REINFORCE Agent
# ===========================================

class REINFORCEAgent:
    def __init__(self, num_states, num_actions, alpha=0.01, gamma=0.99):
        self.num_states = num_states
        self.num_actions = num_actions
        self.alpha = alpha
        self.gamma = gamma

        self.theta = np.zeros((num_states, num_actions))

    def softmax(self, prefs):
        prefs = prefs - np.max(prefs)
        exps = np.exp(prefs)
        return exps / np.sum(exps)

    def choose_action(self, state):
        probs = self.softmax(self.theta[state])
        action = np.random.choice(self.num_actions, p=probs)
        return action, probs

    def get_action_probs(self, state):
        return self.softmax(self.theta[state])

    def compute_returns(self, rewards):
        G = 0
        returns = []
        for r in reversed(rewards):
            G = r + self.gamma * G
            returns.append(G)
        returns.reverse()
        return np.array(returns)

    def update(self, states, actions, returns):
        for t in range(len(states)):
            s = states[t]
            a = actions[t]
            Gt = returns[t]

            probs = self.softmax(self.theta[s])
            grad = -probs
            grad[a] += 1

            self.theta[s] += self.alpha * Gt * grad

    def train_episode(self, env):
        states, actions, rewards = [], [], []

        state = env.reset()
        done = False

        while not done:
            action, _ = self.choose_action(state)
            next_state, reward, done, _ = env.step(action)

            states.append(state)
            actions.append(action)
            rewards.append(reward)

            state = next_state

        returns = self.compute_returns(rewards)
        self.update(states, actions, returns)

        return sum(rewards), states

# ===========================================
# Training
# ===========================================

def train_reinforce(episodes=500):
    env = GridWorld(size=5)
    agent = REINFORCEAgent(env.state_space, env.action_space, alpha=0.01)

    total_rewards = []
    last_episode_states = None

    for ep in range(episodes):
        G, states_traj = agent.train_episode(env)
        total_rewards.append(G)

        if ep == episodes - 1:
            last_episode_states = states_traj

        if (ep + 1) % 50 == 0:
            print(f"Episode {ep+1}: Avg return (last 50) = {np.mean(total_rewards[-50:]):.2f}")

    return agent, total_rewards, last_episode_states, env

# ===========================================
# Visualization 1: Learning Curve
# ===========================================

def plot_learning_curve(rewards):
    plt.figure(figsize=(10,5))
    plt.plot(rewards)
    plt.title("Learning Curve of REINFORCE Agent")
    plt.xlabel("Episodes")
    plt.ylabel("Total Return")
    plt.grid(True)
    plt.show()

# ===========================================
# Visualization 2: State Value Heatmap
# ===========================================

def plot_state_value_heatmap(agent, env):
    values = np.zeros(env.state_space)

    # Estimate value by sampling actions according to learned policy
    for s in range(env.state_space):
        probs = agent.get_action_probs(s)
        values[s] = np.max(probs)   #-use max prob as proxy

    heatmap = values.reshape(env.size, env.size)

    plt.figure(figsize=(6,6))
    plt.imshow(heatmap, cmap="viridis")
    plt.colorbar(label="State value estimate")
    plt.title("Learned State Value Heatmap")
    plt.show()

# ===========================================
# Visualization 3: Plot trajectory of final episode
# ===========================================

def plot_trajectory(states, env):
    coords = [env.state_to_pos(s) for s in states]
    xs = [c[1] for c in coords]
    ys = [c[0] for c in coords]

    plt.figure(figsize=(6,6))
    plt.plot(xs, ys, marker="o")
    plt.gca().invert_yaxis()  # easier to see grid
    plt.grid(True)
    plt.title("Trajectory of final episode")
    plt.xlabel("Column")
    plt.ylabel("Row")
    plt.show()

# ===========================================
# Run training and visualize
# ===========================================

agent, rewards, final_states, env = train_reinforce(episodes=500)

plot_learning_curve(rewards)
plot_state_value_heatmap(agent, env)
plot_trajectory(final_states, env)