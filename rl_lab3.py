# -*- coding: utf-8 -*-
"""RL_Lab3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M1Rb08i160JfdT5TVH1siI4biB1_G1qD
"""

import numpy as np
import matplotlib.pyplot as plt

# --- Step 1: Define MDP ---

states = ['A', 'B', 'C', 'D']
actions = ['left', 'right']

# Transition probabilities (deterministic)
transitions = {
    'A': {'left': 'A', 'right': 'B'},
    'B': {'left': 'A', 'right': 'C'},
    'C': {'left': 'B', 'right': 'D'},
    'D': {'left': 'C', 'right': 'D'}
}

# Rewards
rewards = {
    ('A', 'right', 'B'): 0,
    ('B', 'right', 'C'): 0,
    ('C', 'right', 'D'): 1,  # Goal reward
    ('D', 'right', 'D'): 0,
    ('A', 'left', 'A'): 0,
    ('B', 'left', 'A'): 0,
    ('C', 'left', 'B'): 0,
    ('D', 'left', 'C'): 0
}

gamma = 0.9
theta = 1e-4
V = {s: 0 for s in states}

# --- Step 2: Value Iteration ---

def value_iteration():
    while True:
        delta = 0
        for s in states:
            v = V[s]
            q_values = []
            for a in actions:
                next_state = transitions[s][a]
                r = rewards.get((s, a, next_state), 0)
                q = r + gamma * V[next_state]
                q_values.append(q)
            V[s] = max(q_values)
            delta = max(delta, abs(v - V[s]))
        if delta < theta:
            break

def extract_policy():
    policy = {}
    for s in states:
        best_action = None
        best_value = float('-inf')
        for a in actions:
            next_state = transitions[s][a]
            r = rewards.get((s, a, next_state), 0)
            value = r + gamma * V[next_state]
            if value > best_value:
                best_value = value
                best_action = a
        policy[s] = best_action
    return policy

# Run
value_iteration()
optimal_policy = extract_policy()

print("State Values:")
for s in states:
    print(f"{s}: {V[s]:.4f}")

print("\nOptimal Policy:")
for s, a in optimal_policy.items():
    print(f"{s} â†’ {a}")

# --- Step 3: Visualization ---

# Create numeric mapping for visualization
state_positions = {'A': 0, 'B': 1, 'C': 2, 'D': 3}
values = [V[s] for s in states]

fig, ax = plt.subplots(figsize=(8, 2))
im = ax.imshow([values], cmap='viridis', aspect='auto')

# Show values on cells
for i, s in enumerate(states):
    ax.text(i, 0, f'{s}\n{V[s]:.2f}', ha='center', va='center', color='white', fontsize=12)

# Draw arrows for policy
for i, s in enumerate(states):
    a = optimal_policy[s]
    if a == 'right' and i < len(states)-1:
        ax.arrow(i, 0, 0.7, 0, head_width=0.1, head_length=0.2, fc='white', ec='white')
    elif a == 'left' and i > 0:
        ax.arrow(i, 0, -0.7, 0, head_width=0.1, head_length=0.2, fc='white', ec='white')

ax.set_xticks(range(len(states)))
ax.set_xticklabels(states)
ax.set_yticks([])
ax.set_title("MDP Value Iteration Visualization", fontsize=14)
plt.colorbar(im, ax=ax, orientation='vertical', label='State Value')
plt.show()